{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 4. Streaming Data: Publication and Ingest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 싸이그래머 / CloudΨ - DS on GCP [1]\n",
    "* 김무성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 차례 \n",
    "* 준비단계    \n",
    "* Designing the Event Feed\n",
    "* Time Correction\n",
    "* Apache Beam/Cloud Dataflow\n",
    "    - Parsing Airports Data\n",
    "    - Adding Time Zone Information\n",
    "    - Converting Times to UTC\n",
    "    - Correcting Dates\n",
    "    - Creating Events\n",
    "    - Running the Pipeline in the Cloud\n",
    "* Publishing an Event Stream to Cloud Pub/Sub\n",
    "    - Get Records to Publish\n",
    "    - Paging Through Records\n",
    "    - Building a Batch of Events\n",
    "    - Publishing a Batch of Events\n",
    "* Real-Time Stream Processing\n",
    "    - Streaming in Java Dataflow\n",
    "        - Windowing a pipeline\n",
    "        - Streaming aggregation\n",
    "        - Co-join by key\n",
    "    - Executing the Stream Processing\n",
    "    - Analyzing Streaming Data in BigQuery\n",
    "    - Real-Time Dashboard\n",
    "* Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 참고 \n",
    "* Getting started with Google Cloud Training Material - 2018 - https://www.slideshare.net/jkbaseer/getting-started-with-google-cloud-training-material-2018\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 준비단계"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Google Cloud Shell로 이동 \n",
    "* https://console.cloud.google.com/\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 실습 1. Batch processing in DataFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch processing in DataFlow\n",
    "\n",
    "1. Setup:\n",
    "   ```shell\n",
    "   ./install_packages.sh\n",
    "   ```\n",
    "   <br><br>\n",
    "   \n",
    "2. Parsing airports data:\n",
    "\t```shell\n",
    "\tcd 04_streaming/simulate\n",
    "\t./install_packages.sh\n",
    "\t./df01.py\n",
    "\thead extracted_airports-00000*\n",
    "\trm extracted_airports-*\n",
    "\t```\n",
    "    <br><br>\n",
    "    \n",
    "3. Adding timezone information:\n",
    "\t```shell\n",
    "\t./df02.py\n",
    "\thead airports_with_tz-00000*\n",
    "\trm airports_with_tz-*\n",
    "\t```\n",
    "    <br><br>\n",
    "    \n",
    "4. Converting times to UTC:\n",
    "\t```shell\n",
    "\t./df03.py\n",
    "\thead -3 all_flights-00000*\n",
    "\t```\n",
    "    <br><br>\n",
    "    \n",
    "5. Correcting dates:\n",
    "\t```shell\n",
    "\t./df04.py\n",
    "\thead -3 all_flights-00000*\n",
    "\trm all_flights-*\n",
    "\t```\n",
    "    <br><br>\n",
    "    \n",
    "6. Create events:\n",
    "\t```shell\n",
    "\t./df05.py\n",
    "\thead -3 all_events-00000*\n",
    "\trm all_events-*\n",
    "\t```  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Designing the Event Feed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap01.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time Correction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apache Beam/Cloud Dataflow\n",
    "* Parsing Airports Data\n",
    "* Adding Time Zone Information\n",
    "* Converting Times to UTC\n",
    "* Correcting Dates\n",
    "* Creating Events\n",
    "* Running the Pipeline in the Cloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap02.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing Airports Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding Time Zone Information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting Times to UTC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correcting Dates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Events"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 실습 2. Pipeline on GCP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Go to the GCP web console, API & Services section and enable the Dataflow API.<br><br>\n",
    "\n",
    "2. In CloudShell, type:\n",
    "\t```shell\n",
    "\tbq mk flights\n",
    "\tgsutil cp airports.csv.gz gs://<BUCKET-NAME>/flights/airports/airports.csv.gz\n",
    "\t./df06.py -p $DEVSHELL_PROJECT_ID -b <BUCKETNAME> \n",
    "\t```\n",
    "    <br>\n",
    "3. Go to the GCP web console and wait for the Dataflow ch04timecorr job to finish. It might take several<br><br>\n",
    "\n",
    "4. Then, navigate to the BigQuery console and type in:\n",
    "\t```shell\n",
    "\t\t\tSELECT\n",
    "\t\t\t  ORIGIN,\n",
    "\t\t\t  DEP_TIME,\n",
    "\t\t\t  DEP_DELAY,\n",
    "\t\t\t  DEST,\n",
    "\t\t\t  ARR_TIME,\n",
    "\t\t\t  ARR_DELAY,\n",
    "\t\t\t  NOTIFY_TIME\n",
    "\t\t\tFROM\n",
    "\t\t\t  flights.simevents\n",
    "\t\t\tWHERE\n",
    "\t\t\t  (DEP_DELAY > 15 and ORIGIN = 'SEA') or\n",
    "\t\t\t  (ARR_DELAY > 15 and DEST = 'SEA')\n",
    "\t\t\tORDER BY NOTIFY_TIME ASC\n",
    "\t\t\tLIMIT\n",
    "\t\t\t  10\n",
    "\t```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the Pipeline in the Cloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap03.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap04.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Publishing an Event Stream to Cloud Pub/Sub\n",
    "* Get Records to Publish\n",
    "* Paging Through Records\n",
    "* Building a Batch of Events\n",
    "* Publishing a Batch of Events"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap05.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap06.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Records to Publish"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paging Through Records"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Batch of Events"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Publishing a Batch of Events"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 실습 3. Stream processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. In CloudShell, follow the OAuth2 workflow so that the python script can run code on your behalf:\n",
    "\t```shell\n",
    "\tgcloud auth application-default login\n",
    "\t```\n",
    "    <br>\n",
    "2. Run\n",
    "\t```shell\n",
    "\tpython ./simulate.py --startTime '2015-05-01 00:00:00 UTC' --endTime '2015-05-04 00:00:00 UTC' --speedFactor=30 --project $DEVSHELL_PROJECT_ID\n",
    "    ```\n",
    "    <br>\n",
    "3. In another CloudShell tab, run:\n",
    "\t```shell\n",
    "\tcd 04_streaming/process\n",
    "\t./run_on_cloud.sh <BUCKET-NAME>\n",
    "\t```\n",
    "    <br>\n",
    "4. Go to the GCP web console in the Dataflow section and monitor the job.<br><br>\n",
    "\n",
    "5. Once you see events being written into BigQuery, you can query them from the BigQuery console:\n",
    "\t\t\t```shell\n",
    "\t\t\t#standardsql\n",
    "\t\t\tSELECT\n",
    "\t\t\t  *\n",
    "\t\t\tFROM\n",
    "\t\t\t  `flights.streaming_delays`\n",
    "\t\t\tWHERE\n",
    "\t\t\t  airport = 'DEN'\n",
    "\t\t\tORDER BY\n",
    "\t\t\t  timestamp DESC\n",
    "\t\t\t```\n",
    "            <br>\n",
    "\n",
    "6. In BigQuery, run this query and save this as a view:\n",
    "\t```shell\n",
    "\t\t#standardSQL\n",
    "\t\tSELECT\n",
    "\t\t  airport,\n",
    "\t\t  last[safe_OFFSET(0)].*,\n",
    "\t\t  CONCAT(CAST(last[safe_OFFSET(0)].latitude AS STRING), \",\", CAST(last[safe_OFFSET(0)].longitude AS STRING)) AS location\n",
    "\t\tFROM (\n",
    "\t\t  SELECT\n",
    "\t\t    airport,\n",
    "\t\t    ARRAY_AGG(STRUCT(arr_delay,\n",
    "\t\t        dep_delay,\n",
    "\t\t        timestamp,\n",
    "\t\t        latitude,\n",
    "\t\t        longitude,\n",
    "\t\t        num_flights)\n",
    "\t\t    ORDER BY\n",
    "\t\t      timestamp DESC\n",
    "\t\t    LIMIT\n",
    "\t\t      1) last\n",
    "\t\t  FROM\n",
    "\t\t    `flights.streaming_delays`\n",
    "\t\t  GROUP BY\n",
    "\t\t    airport )\n",
    "\t```   \n",
    "    <br>\n",
    "7. Follow the steps in the chapter to connect to Data Studio and create a GeoMap.<br><br>\n",
    "\n",
    "8. Stop the simulation program in CloudShell.<br><br>\n",
    "\n",
    "9. From the GCP web console, stop the Dataflow streaming pipeline.<br><br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Real-Time Stream Processing\n",
    "* Streaming in Java Dataflow\n",
    "    - Windowing a pipeline\n",
    "    - Streaming aggregation\n",
    "    - Co-join by key\n",
    "* Executing the Stream Processing\n",
    "* Analyzing Streaming Data in BigQuery\n",
    "* Real-Time Dashboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap07.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Streaming in Java Dataflow\n",
    "* Windowing a pipeline\n",
    "* Streaming aggregation\n",
    "* Co-join by key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Windowing a pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Streaming aggregation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Co-join by key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Executing the Stream Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap08.png\" width=1000 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing Streaming Data in BigQuery"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap09.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap10.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real-Time Dashboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap11.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap12.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 참고자료\n",
    "* [1] Data Science on the Google Cloud Platform: Implementing End-to-End Real-Time Data Pipelines: From Ingest to Machine Learning - https://www.amazon.com/Data-Science-Google-Cloud-Platform/dp/1491974567\n",
    "* [2] Book github - https://github.com/GoogleCloudPlatform/data-science-on-gcp"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
